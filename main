import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributions as distributions
import torch.nn.functional as F
import numpy as np
from gymnasium import spaces

class SymlogActivation(torch.nn.Module):
    def forward(self, x):
        return torch.sign(x) * torch.log(torch.abs(x) + 1)

class SymexpActivation(torch.nn.Module):
    def forward(self, x):
        return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)


class HolographicAssociativeMemory(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(HolographicAssociativeMemory, self).__init__()
        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))
        
    def forward(self, x):
        x = x @ self.weight.T
        return x



class HolographicTransformerNetwork(nn.Module):
    def __init__(self, input_size, output_size, hidden_size):
        super(HolographicTransformerNetwork, self).__init__()
        self.ham1 = HolographicAssociativeMemory(input_size, hidden_size)
        self.symlog = SymlogActivation()
        self.ham2 = HolographicAssociativeMemory(hidden_size, hidden_size)
        self.symexp = SymexpActivation()
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        #x = self.ham1(x)
        #x = self.ham2(x)
        #x = self.fc(x)
        x = self.symlog(self.ham1(x))
        x = self.symexp(self.ham2(x))
        x = self.fc(x)
        return F.softmax(x, dim=1)


def normalize_vectors(vectors):
    norms = torch.norm(vectors, dim=1, keepdim=True)
    return vectors / norms


def train(env, model, optimizer, gamma, num_episodes, device):
    """
    Train a policy network using the REINFORCE algorithm.

    Args:
        env (gym.Env): The environment to train the agent in.
        model (nn.Module): The policy network.
        optimizer (optim.Optimizer): The optimizer to use for training.
        gamma (float): The discount factor.
        num_episodes (int): The number of episodes to train for.
        device (str): The device to use for training.

    Returns:
        A list of the total rewards obtained for each episode.
    """
    total_rewards = []
    threshold = 500

    for i in range(num_episodes):
        state = torch.tensor(env.reset()[0], dtype=torch.float32).to(device)
        done = False
        episode_reward = 0
        log_probs = []
        rewards = []

        while not done:
            state = state.clone().detach().requires_grad_(True).unsqueeze(0)
            action, log_prob = select_action(state, model)
            step_result = env.step(action.item())
            next_state, reward, done = step_result[:3]
            next_state = torch.tensor(next_state, dtype=torch.float32).to(device)
            reward = torch.tensor(reward, dtype=torch.float32).to(device)

            log_probs.append(log_prob)
            rewards.append(reward)
            episode_reward += reward.item()

            state = next_state
            if episode_reward >= threshold:
                print(f"Solved after {i+1} episodes!")
                break

        total_rewards.append(episode_reward)
        print(f"Episode {i+1}: reward = {episode_reward:.2f}")

        # Compute the expected returns for each time step
        Gt = torch.zeros(1, 1, dtype=torch.float32, device=device)
        returns = []
        for reward in reversed(rewards):
            Gt = gamma * Gt + reward
            returns.append(Gt)
        returns.reverse()
        returns = torch.cat(returns)

        # Compute the loss and update the model parameters
        log_probs = torch.cat(log_probs)
        loss = -(log_probs * returns).sum()
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
        optimizer.step()

        if i >= 100:
            avg_reward = sum(total_rewards[-100:]) / 100
            if avg_reward >= threshold:
                print(f"Solved after {i+1} episodes!")
                break

    return total_rewards


def select_action(state, model):
    state = state.clone().detach().requires_grad_(True)
    action_probs = model(state)
    action_probs = F.softmax(action_probs, dim=1)  # apply softmax to convert logits to probabilities
    dist = distributions.Categorical(action_probs)
    action = dist.sample()
    return action, dist.log_prob(action)

def main():
    env = gym.make('CartPole-v1')#, render_mode='human')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)
    env.observation_space = observation_space
    input_size = env.observation_space.shape[0]
    output_size = env.action_space.n
    hidden_size = 32
    model = HolographicTransformerNetwork(input_size, output_size, hidden_size)
    optimizer = optim.Adam(model.parameters(), lr=0.0001)
    gamma = .95
    num_episodes = 10000

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    total = train(env, model, optimizer, gamma, num_episodes, device)
    #print(total)
        
    torch.save(model, 'model.pt')

    env.close() 

if __name__ == '__main__':
    print(torch.cuda.is_available())
    main()
